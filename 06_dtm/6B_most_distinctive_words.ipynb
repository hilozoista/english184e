{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most distinctive words\n",
    "\n",
    "Following on [the previous notebook](6A_document_term_matrices.ipynb)...\n",
    "\n",
    "How can we find the words directly which distinguish Republican and Democrat States of the Union? Or between pre- and post-war America?\n",
    "\n",
    "\n",
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some things\n",
    "import os\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set text folder and metadata path\n",
    "# (If you don't have this corpus, please download it here): https://www.dropbox.com/sh/xd854hgyvbysqlm/AAAhbS6r7MFe4SVg1BFuuMTCa?dl=1\n",
    "\n",
    "text_folder = '../corpora/peregrine'\n",
    "path_to_metadata='../corpora/peregrine/peregrine.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each of the filenames\n",
    "\n",
    "def make_dtm(text_folder,n_top_words=1000,normalize=False):\n",
    "    # get stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords=set(stopwords.words('english'))\n",
    "\n",
    "    # make an empty results list\n",
    "    all_results = []\n",
    "\n",
    "    # make a count for all words\n",
    "    from collections import Counter\n",
    "    all_counts = Counter()\n",
    "\n",
    "    # for each filename\n",
    "    filenames=sorted(os.listdir(text_folder))\n",
    "    for i,fn in enumerate(filenames):\n",
    "        if not i%10: print('>> looping through #',i,'of',len(filenames),'files:',fn)\n",
    "        # make sure is a text file\n",
    "        if not fn.endswith('.txt'): continue\n",
    "        \n",
    "        # full path\n",
    "        full_path = os.path.join(text_folder,fn)\n",
    "\n",
    "        # open the file\n",
    "        with open(full_path) as file:\n",
    "            txt=file.read()\n",
    "\n",
    "        # make a blob\n",
    "        blob = TextBlob(txt.lower())\n",
    "\n",
    "        # make a result dictionary\n",
    "        text_result = {}\n",
    "\n",
    "        # set the filename\n",
    "        text_result['fn']=fn\n",
    "\n",
    "        # loop over the word counts\n",
    "        num_words = len(blob.words)\n",
    "\n",
    "        # for each word,count pair in the blob.word_counts dictionary...\n",
    "        for word,count in blob.word_counts.items():\n",
    "            # is the word in the stopwords?\n",
    "            if word in stopwords: continue  \n",
    "\n",
    "            # is the word a punctuation?\n",
    "            if not word[0].isalpha(): continue\n",
    "            \n",
    "            # set the normalized version\n",
    "            if normalize:\n",
    "                # get the term frequency (count divided by number of words)\n",
    "                tf = count / num_words\n",
    "\n",
    "                # set the term frequency result to the key 'word' in the text_result dictionary\n",
    "                text_result[word] = tf\n",
    "            else:\n",
    "                # set the count as a result\n",
    "                text_result[word] = count\n",
    "\n",
    "            # add the count to the dictionary of counts for all words\n",
    "            all_counts[word]+=count\n",
    "\n",
    "        # add results\n",
    "        all_results.append(text_result)\n",
    "    \n",
    "    # Get the most frequent words\n",
    "    most_common_words_plus_counts = all_counts.most_common(n_top_words)\n",
    "    \n",
    "    # Get only the words\n",
    "    word_columns = []\n",
    "    for word,count in most_common_words_plus_counts:\n",
    "        word_columns.append(word)\n",
    "    \n",
    "    # Get columns\n",
    "    columns=[]\n",
    "    columns.append('fn')\n",
    "    columns.extend(word_columns)\n",
    "    \n",
    "    # Make dataframe\n",
    "    df = pd.DataFrame(all_results, columns=columns).set_index('fn').fillna(0)\n",
    "    \n",
    "    # return dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the document term matrix\n",
    "dtm = make_dtm(text_folder,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata for this corpus\n",
    "df_meta = pd.read_csv(path_to_metadata).set_index('fn')\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the metadata\n",
    "dtm_meta=df_meta.merge(dtm,on='fn')\n",
    "dtm_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most distinctive words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Difference of means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta.groupby('season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dtm_meta.groupby('season').mean().T * 1000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta_T = dtm_meta.groupby('Party').mean().T * 1000\n",
    "dtm_meta_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta_T['D-R']=dtm_meta_T['Democrat'] - dtm_meta_T['Republican']\n",
    "round(dtm_meta_T,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dtm_meta_T.sort_values('D-R'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta.boxplot('government',by='Party',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta.boxplot('war',by='Party',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta.sort_values(\"n't\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by government\n",
    "dtm_meta.sort_values('government',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is Nixon using government so much?\n",
    "nixon_path = os.path.join(text_folder, '1971.Nixon.txt')\n",
    "print(nixon_path)\n",
    "\n",
    "# Open the file\n",
    "with open(nixon_path) as file:\n",
    "    nixon_txt=file.read()\n",
    "    \n",
    "# make nltk version of the text (useful for concordance)\n",
    "import nltk\n",
    "nixon_words = nltk.word_tokenize(nixon_txt)\n",
    "nixon_nltk = nltk.text.Text(nixon_words)\n",
    "\n",
    "# get concordance\n",
    "nixon_nltk.concordance('government',width=100,lines=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance(text_folder,filename,word,width=100,lines=1000):\n",
    "    # Get the path\n",
    "    text_path = os.path.join(text_folder, filename)\n",
    "    print(text_path)\n",
    "\n",
    "    # Open the file\n",
    "    with open(text_path) as file:\n",
    "        text_txt=file.read()\n",
    "\n",
    "    # make nltk version of the text (useful for concordance)\n",
    "    import nltk\n",
    "    text_words = nltk.word_tokenize(text_txt)\n",
    "    text_nltk = nltk.text.Text(text_words)\n",
    "\n",
    "    # get concordance\n",
    "    text_nltk.concordance(word,width=width,lines=lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance(text_folder,'1900.McKinley.txt',\"islands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta.boxplot('war',by='Party',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) TF-IDF\n",
    "\n",
    "#### TF: Term Frequency\n",
    "\n",
    "<center><img src=\"https://latex.codecogs.com/png.latex?TF = \\frac{n_w}{n_d}\"></center>\n",
    "\n",
    "Where:\n",
    "* *Nw* is the number of times a given word *w* appears in a document.\n",
    "* *Nd* is the number of words in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a given word?\n",
    "given_word='jobs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have that calculated here:\n",
    "tf_series = dtm[given_word]\n",
    "tf_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "<center><img src=\"https://latex.codecogs.com/png.latex?IDF = \\log \\left( \\frac{c_d}{i_d} \\right)\"></center>\n",
    "\n",
    "Where:\n",
    "* <img src=\"https://latex.codecogs.com/png.latex?{c_d}\"> is the count of documents in the corpus.\n",
    "* <img src=\"https://latex.codecogs.com/png.latex?{i_d}\"> = is the number of documents in which that word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of documents\n",
    "num_docs = len(dtm)\n",
    "num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of documents a given word appears\n",
    "dtm[dtm[given_word]>0][given_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_with_word=len(dtm[dtm[given_word]>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idf = np.log(num_docs/num_docs_with_word)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_series = tf_series * idf\n",
    "tfidf_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'tfidf':tfidf_series, 'tf':tf_series}).plot(x='tf',y='tfidf',kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make\n",
    "def to_tfidf(dtm):\n",
    "    # list of dictionaries\n",
    "    dtm_tfidf = pd.DataFrame()\n",
    "    \n",
    "    for word in dtm.columns:\n",
    "        # tf\n",
    "        tf_series = dtm[word]\n",
    "        \n",
    "        # idf\n",
    "        num_docs = len(dtm)\n",
    "        num_docs_with_word=len(dtm[dtm[word]>0])\n",
    "        idf=np.log(num_docs/num_docs_with_word)\n",
    "        \n",
    "        # tfidf\n",
    "        tfidf_series = tf_series * idf\n",
    "        dtm_tfidf[word]=tfidf_series\n",
    "    \n",
    "    return dtm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfidf=to_tfidf(dtm)\n",
    "dtm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='america'\n",
    "dtm[word].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfidf[word].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'tf':dtm[word], 'tfidf':dtm_tfidf[word]}).plot(x='tf',y='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn='2002.Bush.txt'\n",
    "dtm_tfidf.loc[fn].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.loc[fn].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn='2017.Trump.txt'\n",
    "dtm_tfidf.loc[fn].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 10\n",
    "for index in reversed(dtm_tfidf.index):\n",
    "    # get row for this index\n",
    "    row=dtm_tfidf.loc[index]\n",
    "    \n",
    "    # get the lagest words\n",
    "    top_words_series=row.nlargest(n_words)\n",
    "    top_words_list=list(top_words_series.index)\n",
    "    top_words_str=', '.join(top_words_list)\n",
    "    \n",
    "    # print\n",
    "    print('##',index.upper())\n",
    "    print(top_words_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Fisher's exact test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this we need a document-term matrix *of raw counts*\n",
    "dtm_counts = make_dtm(text_folder,normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_counts_meta = df_meta.merge(dtm_counts,on='fn')\n",
    "dtm_counts_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs=dtm_counts[df_meta.Party == 'Republican']\n",
    "Ds=dtm_counts[df_meta.Party == 'Democrat']\n",
    "Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='immigration'\n",
    "sum_word_Rs = Rs[word].sum()\n",
    "sum_word_Ds = Ds[word].sum()\n",
    "\n",
    "print(sum_word_Rs,sum_word_Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_allword_Rs=Rs.sum().sum()\n",
    "sum_allword_Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_allword_Ds=Ds.sum().sum()\n",
    "sum_allword_Ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_notword_Rs = sum_allword_Rs - sum_word_Rs\n",
    "sum_notword_Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_notword_Ds = sum_allword_Ds - sum_word_Ds\n",
    "sum_notword_Ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = [\n",
    "    [sum_word_Rs, sum_notword_Rs],\n",
    "    [sum_word_Ds, sum_notword_Ds]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "oddsratio, pvalue = fisher_exact(contingency_table)\n",
    "oddsratio, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Stacking\" a DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_counts.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stacked = dtm_counts.stack().reset_index()\n",
    "dtm_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stacked.columns = ['fn','word','count']\n",
    "dtm_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table back to unstacked original form\n",
    "dtm_stacked.pivot(index='fn',columns='word',values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stacked DTM with meta\n",
    "dtm_stacked_meta = df_meta.merge(dtm_stacked,on='fn')\n",
    "dtm_stacked_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_Rs=dtm_stacked_meta.query('word == \"government\" & Party == \"Republican\"')['count'].sum()\n",
    "num_word_notRs=dtm_stacked_meta.query('word == \"government\" & Party != \"Republican\"')['count'].sum()\n",
    "num_notword_Rs=dtm_stacked_meta.query('word != \"government\" & Party == \"Republican\"')['count'].sum()\n",
    "num_notword_notRs=dtm_stacked_meta.query('word != \"government\" & Party != \"Republican\"')['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = [\n",
    "    [num_word_Rs, num_word_notRs],\n",
    "    [num_notword_Rs, num_notword_notRs]\n",
    "]\n",
    "fisher_exact(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try every word!\n",
    "result_list=[]\n",
    "\n",
    "party='Republican'\n",
    "for word in dtm_stacked_meta['word'].unique():\n",
    "    num_word_Rs=dtm_stacked_meta.query('word == \"'+word+'\" & Party == \"'+party+'\"')['count'].sum()\n",
    "    num_word_notRs=dtm_stacked_meta.query('word == \"'+word+'\" & Party != \"'+party+'\"')['count'].sum()\n",
    "    num_notword_Rs=dtm_stacked_meta.query('word != \"'+word+'\" & Party == \"'+party+'\"')['count'].sum()\n",
    "    num_notword_notRs=dtm_stacked_meta.query('word != \"'+word+'\" & Party != \"'+party+'\"')['count'].sum()\n",
    "    contingency_table = [\n",
    "        [num_word_Rs, num_word_notRs],\n",
    "        [num_notword_Rs, num_notword_notRs]\n",
    "    ]\n",
    "    oddsratio,pvalue=fisher_exact(contingency_table)\n",
    "    if oddsratio>2 and pvalue<0.05:\n",
    "        print('{oddsratio} to 1 = the odds of \"{word}\" appearing in {party} (vs. non-{party}) texts'.format(\n",
    "            word=word,party=party,oddsratio=round(oddsratio,2)))\n",
    "\n",
    "    result_dict={}\n",
    "    result_dict['word']=word\n",
    "    result_dict['oddsratio']=oddsratio\n",
    "    result_dict['pvalue']=pvalue\n",
    "    result_dict['group']=party\n",
    "    result_list.append(result_dict)\n",
    "\n",
    "df_mdw = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdw[df_mdw.group=='Democrat'].sort_values('oddsratio',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdw[df_mdw.group=='Republican'].sort_values('oddsratio',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest?\n",
    "dtm['forest'].nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs=dtm[df_meta.Party=='Republican']\n",
    "Ds=dtm[df_meta.Party=='Democrat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='forest'\n",
    "x=Rs[word]\n",
    "y=Ds[word]\n",
    "mannwhitneyu(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtm_stacked_meta.query('Party == \"Republican\" and word==\"forest\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mannwhitney(group1,group2,words=None):\n",
    "    if not words:\n",
    "        words = set(group1.columns) & set(group2.columns)\n",
    "    \n",
    "    result_list=[]\n",
    "    for word in words:\n",
    "        x=group1[word]\n",
    "        y=group2[word]\n",
    "        \n",
    "        mwU, pvalue = mannwhitneyu(x,y)\n",
    "    \n",
    "        result_dict={}\n",
    "        result_dict['word']=word\n",
    "        result_dict['mannwhitney_U']=mwU\n",
    "        result_dict['mannwhitney_pvalue']=pvalue\n",
    "        result_list.append(result_dict)\n",
    "        \n",
    "    return pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mannwhitney=compute_mannwhitney(Rs,Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Republicans\n",
    "df_mannwhitney.sort_values('mannwhitney_U',ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Democrats\n",
    "df_mannwhitney.sort_values('mannwhitney_U',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mannwhitney[df_mannwhitney.word==\"iraq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdw[df_mdw.word==\"iraq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_meta_T.loc['iraq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdw.merge(df_mannwhitney,on='word').plot(x='oddsratio',logx=True,y='mannwhitney_U',kind='scatter',figsize=(8,8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
